# Mini GPT From Scratch

## Overview
Built a GPT-style transformer from scratch using PyTorch.

## Features
- Causal self-attention
- Multi-head attention
- Transformer blocks
- Streaming OpenWebText dataset
- Cosine LR scheduling
- Gradient clipping
- CUDA training

## Architecture
Explain blocks here.

## Training Results
step 28500
loss 4.6658
perplexity 106.25

## How to Run
pip install -r requirements.txt
python src/train.py