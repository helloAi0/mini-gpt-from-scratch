# ğŸ§  Mini GPT From Scratch

A minimal GPT-style language model implemented from scratch using PyTorch.

This project recreates the core components of a Transformer-based language model, including:

- Token embeddings  
- Positional embeddings  
- Multi-head self-attention  
- Transformer blocks  
- Layer normalization  
- Training loop with gradient clipping  
- Text generation  

---

## ğŸ“‚ Project Structure

mini-gpt-from-scratch/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py        # GPT model definition
â”‚   â”œâ”€â”€ attention.py    # Transformer blocks & attention
â”‚   â”œâ”€â”€ dataset.py      # Data loading & batching
â”‚   â”œâ”€â”€ train.py        # Training loop
â”‚   â”œâ”€â”€ generate.py     # Text generation
â”‚   â””â”€â”€ config.py       # Hyperparameters
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## âš™ï¸ Setup

```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt